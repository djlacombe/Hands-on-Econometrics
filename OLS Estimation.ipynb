{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The OLS Estimator: Theory, Intuition, and Computation\n",
    "The following notebook explains the OLS estiamtor from both a computational and intuitive perspective. Many students struggle with the mathematics and intuition of the OLS estimator. This notebook provides all of the mathematical and technical details of the OLS estimator along with Python code examples to build intuition for one of the most important algorithms in applied econometrics and data science."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Data Generating Process\n",
    "The data generator processes, or DGP, is the starting point for our investigation into the OLS estimator. The DGP is the model that generates our data and generating our own data can facilitate the analysis of the OLS estimator. \n",
    "\n",
    "Another way to think about this is that the research process is the reverse of the DGP process. When engaging in empirical research, we are looking for the most appropriate DGP. In other words, we are looking for a model that could have produced the data that we have at hand. In may cases, we will not be able to completely determine the \"true\" DGP, however the point is to get as close as possible to the formula that generated the data. In practical terms, this means the choice of explanatory variables that could we make should mimic as closely as possible the DGP. Of course, the choice of which explanatory variables to include in our model is just one choice to be made, but it illustrates how the research process is designed to approximate the true DGP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How do we Generate Data?\n",
    "Generating data from scratch is a fairly simple process, but there are a few decisions that have to be made. First, we need to determine the number of observations we want in our sample, usually denoted by $N$. Next, we need to determine the number of explanatory variables that we would like in our model, usually denoted as $k$. In this lesson, we will use a $k$ that is greater than one, i.e., $k >1$. When $k>1$, we have more than one explanatory variable and thus have a multiple regression model. The multiple regression model can be expressed using matrix algrebra readily, and we will examine these formulas later in the lesson.\n",
    "\n",
    "First, we need to import Numpy into our Python session. Numpy is a library that adds various matrix and linear algebra routines that are necessary to compute the quantities of interest that we will need later on in the lesson. We can import Numpy with the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(123)  # This command sets the random seed so that the results here will match your results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also import scipy.stats for some of the statistical calculations that we will perform later on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as st\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to specify the number of observations and number of explanatory variables that we will use in our DGP. It should be noted that these choices are arbitrary, and both of these value can be edited and the notebook re-run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 2       # Number of explanatory variables in our regression model (excluding constant--we will get to this later)\n",
    "N = 1000   # Number of observations in our dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actual regression model indicates that we have $k = 2$ explanatory variables. Mathematically, this is defined as follows:\n",
    "\n",
    "$y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\varepsilon$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coefficient $\\beta_0$ refers to the constant or intercept term, while $\\beta_1$ and $\\beta_2$ are the coefficients on the two explanatory variables, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to generate values for each $X$ variable. The choice of distributional from for these variables is also arbitrary, as the distribution of the explanatory variables has no effect on the estimates of the coefficients  that we obtain from the OLS procedure. In other words, there is no distributional assumption regarding the explanatory variables when using OLS. \n",
    "\n",
    "With this caveat in mind, we now generate random values for our explanatory variables. We also need to keep in mind the dimension of the variables we will generate. Usually, the matrix of explanatory variables has dimension $N \\times k$ where $N$ is the number of rows in our dataset (i.e.,the number of observations) and $k$ is the number of columns (i.e., the number of variables) in our dataset. You can also think of this matrix of data as an Excel spreadsheet, where we have $N$ rows (observations) and $k$ columns (number of variables).\n",
    "\n",
    "In this example, we are creating independent $X$'s, which means the correlation between the two variables is zero. We could use a multivariate nomral distribution to generate the explanatory variables with a set correlation between the variables, and the procedure for doing this will be explained in the Conclusion section of this notebook.\n",
    "\n",
    "Lets generate two random column vectors (with dimension $N \\times 1$) to represent the data for our explanatory variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = np.random.randn(N).reshape(-1,1)    # reshape(-1,1) reshapes the vector to a column vector\n",
    "\n",
    "x2 = np.random.randn(N).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print out each of our explanatory variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first explanatiory variable:\n",
      "[[ 0.99734545]\n",
      " [ 0.2829785 ]\n",
      " [-1.50629471]\n",
      " [-0.57860025]\n",
      " [ 1.65143654]]\n"
     ]
    }
   ],
   "source": [
    "print('The first explanatiory variable:')\n",
    "print(x1[1:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The second explanatory variable:\n",
      "[[ 0.56759473]\n",
      " [ 0.71815054]\n",
      " [-0.99938075]\n",
      " [ 0.47489832]\n",
      " [-1.86849981]]\n"
     ]
    }
   ],
   "source": [
    "print('The second explanatory variable:')\n",
    "print(x2[1:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression models usually contain a constant, or intercept term, and we can generate a constant term by creating a Numpy array of ones, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "intercept = np.ones(N).reshape(-1,1)    # Intercept term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have generated the intercept and our two explanatory variables, it is now time to combine all of them into one big matrix $X$. We can accomplish this by horzontially stacking all three vectors toghther:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.hstack((intercept,x1,x2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can examine the matrix $X$ and confirm that the first column is the intercept and the other two columns represent our explanatory variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.          0.99734545  0.56759473]\n",
      " [ 1.          0.2829785   0.71815054]\n",
      " [ 1.         -1.50629471 -0.99938075]\n",
      " [ 1.         -0.57860025  0.47489832]\n",
      " [ 1.          1.65143654 -1.86849981]]\n"
     ]
    }
   ],
   "source": [
    "print(X[1:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have generated our random data, it's time to choose the $\\beta$'s or coefficinets in our model. The multiple regression model is usually written in matrix form as follows:\n",
    "\n",
    "$y = X \\beta + \\varepsilon$\n",
    "\n",
    "We can see that the matrix $X$ is multiplied by our vector of $\\beta$'s and these two must conform per the rules of matrix multiplication. The matrix $X$ has dimension $\\left(N \\times k\\right)$ and $\\beta$ is a vector of dimension $\\left(k \\times 1\\right)$, which results in a column vector of dimension $\\left( N\\times 1\\right)$. \n",
    "\n",
    "This choice for the value of $\\beta$ is arbitrary, and we choose the value 3 for each of our coefficients in the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3]\n",
      " [3]\n",
      " [3]]\n"
     ]
    }
   ],
   "source": [
    "beta = np.array([3,3,3]).reshape(-1,1)\n",
    "print(beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now examine the $X \\beta$ term after performing matrix multiplication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7.69482052]\n",
      " [ 6.00338712]\n",
      " [-4.51702639]\n",
      " [ 2.68889421]\n",
      " [ 2.34881017]]\n"
     ]
    }
   ],
   "source": [
    "xb = X@beta\n",
    "print(xb[1:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have to choose the $\\varepsilon$, or error term. Recall that in the OLS model and according to the Gauss-Markov Theorem, the distribution of the error term doesn't have any effect on OLS's ability to give us the correct coefficient values. We only need the normality assumption if we wish to perform hypothesis testing.\n",
    "\n",
    "If we do wish to do hypothesis testing, then we make the assumption that the error term is normally distributed, with mean $0$ and variance $\\sigma^2$:\n",
    "\n",
    "$\\varepsilon \\sim N\\left(0,\\sigma^2 \\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now generate our random error term. We choose the standard normal distribution, with mean $0$ and standard deviation $1$. Of course, other values for $\\sigma^2$ are possible and we illustrate how to do this in the Conclusion section to this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.20137731]\n",
      " [ 1.09625679]\n",
      " [ 0.86103685]\n",
      " [-1.52036712]\n",
      " [-0.44744016]]\n"
     ]
    }
   ],
   "source": [
    "error = np.random.randn(N,1)\n",
    "print(error[1:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we are ready to generate our dependent variable. Recall that the multiple regression model can be written in matrix form as follows:\n",
    "\n",
    "$y = X \\beta + \\varepsilon$\n",
    "\n",
    "Combining all of the elements that we have so far created, we can now generate our dependent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 6.49344321]\n",
      " [ 7.09964391]\n",
      " [-3.65598954]\n",
      " [ 1.16852709]\n",
      " [ 1.90137001]]\n"
     ]
    }
   ],
   "source": [
    "y = X@beta + error\n",
    "print(y[1:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the matrix of generated data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 6.49344321  1.          0.99734545  0.56759473 -1.20137731]\n",
      " [ 7.09964391  1.          0.2829785   0.71815054  1.09625679]\n",
      " [-3.65598954  1.         -1.50629471 -0.99938075  0.86103685]\n",
      " [ 1.16852709  1.         -0.57860025  0.47489832 -1.52036712]\n",
      " [ 1.90137001  1.          1.65143654 -1.86849981 -0.44744016]]\n"
     ]
    }
   ],
   "source": [
    "data = np.hstack((y,X,error))\n",
    "print(data[1:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first column of our \"data matrix\" is the dependent variable, while columns 2,3, and 4 represent the explanatory variables. The final column is the error term. We can calculate the first value of the dependent variable using the following values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.          0.99734545  0.56759473 -1.20137731]\n"
     ]
    }
   ],
   "source": [
    "print(data[1,1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.4934432121296\n"
     ]
    }
   ],
   "source": [
    "print(3*data[1,1] + 3*data[1,2] + 3*data[1,3] + data[1,4]) # We multiply the intercept term and two explanatory variables by 3.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice that our \"by hand\" calculation of the first value of the dependent vaariable matches what we calculated via matrix algebra. Instead of doing all of these calculations by hand, matrix algebra allows one to perform these calculations regardless of the size of the dataset, which is extremely convenient!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The OLS Estimator\n",
    "\n",
    "We now turn our attention to the OLS estimator. Although we will not formally derive the formula for the OLS estimator, the formula is as follows:\n",
    "\n",
    "$\\hat \\beta  = {\\left( {X'X} \\right)^{ - 1}}X'y$\n",
    "\n",
    "The formula above returns the vector of $\\hat \\beta$ coefficients estimated from our generated data and has dimension $\\left(k \\times 1\\right)$, i.e., one coefficient for each explanatory variable, including the constant term. Let's calculate this quantity in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.00920206]\n",
      " [2.9677746 ]\n",
      " [2.98522671]]\n"
     ]
    }
   ],
   "source": [
    "beta_hat = np.linalg.inv(X.T@X)@X.T@y\n",
    "print(beta_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the $\\beta$'s we used in generating the data were all set to the value 3, so we should expect to see this value when we calculate the coefficeints \"by hand\". You'll note that the estimates from the formula above are very close to the true values that we used to generate the data. It appears that our \"by hand\" OLS calculations are very accurate.\n",
    "\n",
    "In empirical research, we never truly know the true data generating process and the goal is to try and mimic this unknown linear function using regression techniques. The beauty of generating our own data is that we can use all of the formulas used in OLS and calculate everything \"by hand\" and examine each quantity to see how all of the pieces fit together. \n",
    "\n",
    "We have calculated our coefficients but what about statistical significance? We now turn our attention to this topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hypothesis testing is an important part of regression modeling and it is important to see how we can calculate all of the quantities using the standard formulas.\n",
    "\n",
    "One of the first quantitities that needs to be calculated are the residuals from our regression model. Recall that the residuals are calculated as follows:\n",
    "\n",
    "$\\hat ɛ = y - \\hat y$\n",
    "\n",
    "where $y$ is the actual value of the dependent variable and $\\hat y$ is the predicted value of $y$ from our regression model. The predicted values are those values that our model predicts for each value of the explantory variables. Predicted values in our regression model are clauclated as follows:\n",
    "\n",
    "$X  \\hat \\beta$\n",
    "\n",
    "where $\\hat \\beta$ are the regression coefficients calculated via OLS. We can rewrite the formula above to include this information:\n",
    "\n",
    "$\\hat ɛ = y - X \\hat \\beta$\n",
    "\n",
    "The residuals need to be calcualted so that we may obtain an estimate of $\\hat \\sigma^2$. The quantity $\\hat \\sigma^2$ is used to obtain the standard errors of our regression coefficients.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "An important concept in regression modeling is the idea of a sampling distribution. The basic idea is that whenever we engage in empirical research, we obtain one possible dataset out of many possible datasets that could have been collected. For example, the dataset that we obtain could have been collected the day before or the day after, or any other day. If we were to run a separate regression for each dataset that we collect, we would obtain different estimates for the regression coefficients and these coefficients would have a distribution, referred to as the sampling distribution of the OLS estimator. \n",
    "\n",
    "Given that the regressions we run from different datasets that we could have collected form a distribution of coefficeints (i.e., a separate distribution for each coefficient), that sampling distribution of the OLS estimates would also have a standard deviation, which is a measure of how confident we are in the coefficient estimates. A small standard deviation means that the coefficients have been estimated fairly accurately, while a large standard deviation means that the coefficients have been estimated with higher uncertainty. The standard deviation of the sampling disrtibution of the regression coefficients can be affected by the sample size of the data (larger sample sizes decrease the standard error), the value of $\\hat \\sigma^2$ (how tightly grouped the data are around the regression line--smaller values are better), and the variation in our explanatory variable, where more variation is preferred.\n",
    "\n",
    "The formula for calculating the standard error of the regression coefficients is referred to as the variance-covariance matrix. The VC matrix is a $\\left(k \\times k \\right)$ matrix, where the diagonal elements are the variances of the sampling distributons of the coefficient estimates. The formula for the VC matrix is as follows:\n",
    "\n",
    "$VC_{\\hat \\beta} = \\hat \\sigma {\\left( {X'X} \\right)^{ - 1}}$\n",
    "\n",
    "where $\\hat \\sigma^2$ is an estimate of the true value $\\sigma^2$. Since we do not know the true value of $\\sigma^2$, we have to estimate it from the data.\n",
    "\n",
    "The formula for calcualting the value of $\\hat \\sigma^2$ is as follows:\n",
    "\n",
    "$\\hat \\sigma ^2 = \\frac{{\\varepsilon'\\varepsilon }}{{N - k}}$\n",
    "\n",
    "Note that $\\hat \\sigma^2$ will be a scalar, i.e., a single number. Recall that $\\varepsilon$ is a $\\left(N \\times 1\\right)$ vector and it's transpose is a $\\left(1 \\times N\\right)$ vector. When we multiply the transposed $\\left(1 \\times N\\right)$ vector of residuals by the original $\\left(N \\times 1\\right)$ vector of residuals, we obtain a $\\left(1 \\times 1\\right)$ \"matrix\" or single number, which is referred to as a scalar.  \n",
    "\n",
    "Let's calculate this quantity in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our estimate of sigma squared is :[[0.96397663]]\n"
     ]
    }
   ],
   "source": [
    "residual = y - X@beta_hat   #Calculate residual\n",
    "\n",
    "sigma_2 = (residual.T@residual)/(N-k)\n",
    "\n",
    "print(f'Our estimate of sigma squared is :{sigma_2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll note that since we created our error term from a $N\\left(0,1 \\right)$ standard normal distribution, our estimate of $\\sigma^2$ is close to the true value of 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we now have all of the elements to calculate the VC matrix, let's do so in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 9.65537757e-04  3.78520768e-05 -7.57439553e-06]\n",
      " [ 3.78520768e-05  9.63402925e-04  3.14843680e-05]\n",
      " [-7.57439553e-06  3.14843680e-05  1.05136147e-03]]\n"
     ]
    }
   ],
   "source": [
    "VC = sigma_2*np.linalg.inv(X.T@X)\n",
    "print(VC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The diagonal elements of the VC matrix are the variances of the sampling distributions of the OLS estimator. Element (1,1) is the variance for the intercept, while element (2,2) and element (3,3) are the variances for explanatory variables 2 and 3, respectively. \n",
    "\n",
    "In reality, we actually need the standard deviation of these estimates in order to calculate the t-statistics needed for hypothesis testing, which requires taking the square root of the diagonal elements.\n",
    "\n",
    "We can calculate the standard deviation of the sampling distribution of the OLS estimator in Python as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0310731 ]\n",
      " [0.03103873]\n",
      " [0.0324247 ]]\n"
     ]
    }
   ],
   "source": [
    "VC_diagonal = np.sqrt(np.diag(VC)).reshape(k+1,1)\n",
    "print(VC_diagonal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The t-statistics needed for hypothesis testing are simply the values of the estimated coefficients divided by their respective standard errors.\n",
    "\n",
    "Calculating these t-statistics can be accomplished in Python as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[96.84266211]\n",
      " [95.61519896]\n",
      " [92.06642739]]\n"
     ]
    }
   ],
   "source": [
    "t_stats = beta_hat/VC_diagonal\n",
    "print(t_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we now have calculated our t-statistics, we can find their associated p--values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "p_values = st.t.sf(abs(t_stats), df=N-k-1)*2    # Multiply by 2 for a 2 sided test\n",
    "print(p_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final entry in our \"table\" of results are the lower and upper 95% confidence intervals. Recall from basic statistics that the 95% confidence interval for our coefficient estimate can be calculated as follows:\n",
    "\n",
    "$\\hat \\beta \\pm t_{critical} \\times \\operatorname{SE} \\left(\\hat \\beta\\right)$\n",
    "\n",
    "We first need to calculate the critical value, and we assume $\\alpha = 0.05$, which we divide by two in the function call since we are assuming a two-tailed test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.962346236089449\n"
     ]
    }
   ],
   "source": [
    "t_critical = st.t.ppf(q=1-.05/2,df=N-k-1)\n",
    "print(t_critical)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we calculate both the lower and upper bound to obtain the 95% confidence intervals for our coefficient estimates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lower 95% intervals:\n",
      "[[2.94822587]\n",
      " [2.90686586]\n",
      " [2.92159821]]\n",
      "The upper 95% intervals\n",
      "[[3.07017824]\n",
      " [3.02868334]\n",
      " [3.04885521]]\n"
     ]
    }
   ],
   "source": [
    "t_critical = np.abs(t_critical)\n",
    "lower95 = beta_hat - t_critical*VC_diagonal     # t-critical is the critical value from the t-distribution\n",
    "upper95 = beta_hat + t_critical*VC_diagonal     # VC_diagonal are the SE's for each coefficient\n",
    "\n",
    "print('The lower 95% intervals:')\n",
    "print(lower95)\n",
    "print('The upper 95% intervals')\n",
    "print(upper95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the quantities that we have calculated are spread out over this notebok and it's extremely convenient to collect all of these quantities in a pandas Data Frame for easy viewing.\n",
    "\n",
    "We first import pandas and then create a Data Frame with all of our information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Coef.</th>\n",
       "      <th>Std. Error</th>\n",
       "      <th>t-stat</th>\n",
       "      <th>p-value</th>\n",
       "      <th>Lower 95%</th>\n",
       "      <th>Upper 95%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Intercept</th>\n",
       "      <td>3.009</td>\n",
       "      <td>0.031</td>\n",
       "      <td>96.843</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.948</td>\n",
       "      <td>3.070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X1</th>\n",
       "      <td>2.968</td>\n",
       "      <td>0.031</td>\n",
       "      <td>95.615</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.907</td>\n",
       "      <td>3.029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X2</th>\n",
       "      <td>2.985</td>\n",
       "      <td>0.032</td>\n",
       "      <td>92.066</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.922</td>\n",
       "      <td>3.049</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Coef.  Std. Error  t-stat  p-value  Lower 95%  Upper 95%\n",
       "Intercept  3.009       0.031  96.843      0.0      2.948      3.070\n",
       "X1         2.968       0.031  95.615      0.0      2.907      3.029\n",
       "X2         2.985       0.032  92.066      0.0      2.922      3.049"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "table = np.hstack([beta_hat, VC_diagonal, t_stats,p_values,lower95,upper95])\n",
    "\n",
    "df = pd.DataFrame(table, index = ['Intercept','X1', 'X2'], columns = ['Coef.', 'Std. Error', 't-stat','p-value','Lower 95%', 'Upper 95%'])\n",
    "\n",
    "results_dataframe = df.round(decimals=3)    #Round values for easy reading\n",
    "\n",
    "results_dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verification of Our Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, it is inconvenient to calculate all of these statistics by hand (but certaily educational!). Also, it's important to compare our results to an established routine to make sure that we have calcualted everything correctly.\n",
    "\n",
    "We can use the Python package statsmodels to perform a linear regression and compare the results from that routine to the results in our table above.\n",
    "\n",
    "We first import that statsmodel package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we call the OLS function and print out the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.945\n",
      "Model:                            OLS   Adj. R-squared:                  0.945\n",
      "Method:                 Least Squares   F-statistic:                     8534.\n",
      "Date:                Fri, 08 Jul 2022   Prob (F-statistic):               0.00\n",
      "Time:                        09:42:39   Log-Likelihood:                -1399.6\n",
      "No. Observations:                1000   AIC:                             2805.\n",
      "Df Residuals:                     997   BIC:                             2820.\n",
      "Df Model:                           2                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          3.0092      0.031     96.794      0.000       2.948       3.070\n",
      "x1             2.9678      0.031     95.567      0.000       2.907       3.029\n",
      "x2             2.9852      0.032     92.020      0.000       2.922       3.049\n",
      "==============================================================================\n",
      "Omnibus:                        0.675   Durbin-Watson:                   1.893\n",
      "Prob(Omnibus):                  0.713   Jarque-Bera (JB):                0.758\n",
      "Skew:                          -0.049   Prob(JB):                        0.684\n",
      "Kurtosis:                       2.907   Cond. No.                         1.07\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "model = sm.OLS(y, X).fit()\n",
    "\n",
    "print_model = model.summary()\n",
    "\n",
    "print(print_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we compare the results calculated in this notebook to the statsmodels results, we see that they are almost exactly the same, subject to rounding error.\n",
    "\n",
    "# Conclusion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matrix formulation of the OLS routine is a convenient manner in which to calculate the quantities that we normally see from \"black box\" regression software. This notebook is designed to demystify this process to see how these calculations are done \"by hand\" so that students of regression modeling can build intuition regarding how the mathematics translates into Python code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We encourage students to explore this notebook and change some of the quantities to see how these changes affect the OLS estimator. Suggestions include the following:\n",
    "\n",
    "* Decrease the number of observations in the notebook to $N = 25$. How does the decrease in sample size affect the estimates?\n",
    "* Change the standard deviation of the error term using this code:\n",
    "```\n",
    "    n = 100\n",
    "    error = np.random.randn(N,1)*np.sqrt(n) # The n here is the standard deviation of the error term, not the sample size.\n",
    "```\n",
    "How does incresing the standard deviation of the error term affect the coefficient estimates, if at all? Does it change the significance of the coefficient estimates?\n",
    "* The explanatory variables can be generated from a multivariate normal distribution with the folloiwng code:\n",
    "  \n",
    "  ```\n",
    "  mean = np.zeros(2)  # The number of explanatory variable is two, and their mean is zero.\n",
    "  covariance = np.array([[1, .8],\n",
    "                        [.8,1]])\n",
    "  X = np.random.multivariate_normal(mean, covariance,N).reshape(N,2)\n",
    "  ```\n",
    "In this code snippet, we are generating two explanatory variables from a multivariate normal distribution with a mean of zero and standard deviation of 1 (which are on the diagonal of the covariance matrix). Note that the covariance matrix for these random draws is set to 0.8, which indicates that the two variables have a correlation of $\\rho = 0.8$ (the off-diagonal elements of the covariance matrix).\n",
    "\n",
    "Try adjusting the correlation to a large number such as $\\rho = 0.99$. What effect does this have on the model estimates? What happens if we set $\\rho =1$?\n",
    "\n",
    "* Increase the dispersion of the explantory variables generated by using the following code:\n",
    "```\n",
    "  x1 = np.random.randint(0,500,N).reshape(-1,1)\n",
    "  x2 = np.random.randint(0,500,N).reshape(-1,1)\n",
    "```\n",
    "The original code generated the x1 and x2 variables from a standard normal distribution with a mean of 0 and standard deviation of 1. The above code generates a random x1 and x2 from a uniform distribution of integers with a range from 0 to 500. Explore by changing the upper bound to 10, which restricts the dispersion of the explanatory variables. How does this affect the estimates?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6f6588b93aa1c2563828190b273f38bf61794ec88566d94c9551de9b6ed2a57e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
